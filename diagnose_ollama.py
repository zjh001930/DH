#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ollamaè¿æ¥è¯Šæ–­è„šæœ¬
"""

import os
import sys
import requests
import json
from pathlib import Path

# è®¾ç½®ç¯å¢ƒå˜é‡ä½¿ç”¨æœ¬åœ°é…ç½®
os.environ['OLLAMA_API_URL'] = 'http://localhost:11434'
os.environ['LLM_MODEL_NAME'] = 'qwen2.5:3b-instruct'
os.environ['EMBEDDING_MODEL_NAME'] = 'bge-m3'

# æ·»åŠ backendç›®å½•åˆ°Pythonè·¯å¾„
backend_dir = Path(__file__).parent / "backend"
sys.path.insert(0, str(backend_dir))

def check_ollama_connection():
    """æ£€æŸ¥Ollamaè¿æ¥"""
    print("ğŸ” è¯Šæ–­Ollamaè¿æ¥é—®é¢˜")
    print("=" * 50)
    
    # æµ‹è¯•ä¸åŒçš„URL
    urls_to_test = [
        "http://localhost:11434",
        "http://127.0.0.1:11434",
        "http://ollama_service:11434",
        "http://ollama_host:11434"
    ]
    
    for url in urls_to_test:
        print(f"\nğŸ”— æµ‹è¯•è¿æ¥: {url}")
        try:
            # æµ‹è¯•åŸºæœ¬è¿æ¥
            response = requests.get(f"{url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json()
                print(f"âœ… è¿æ¥æˆåŠŸ!")
                print(f"ğŸ“‹ å¯ç”¨æ¨¡å‹:")
                if 'models' in models and models['models']:
                    for model in models['models']:
                        name = model.get('name', 'Unknown')
                        print(f"   - {name}")
                        if 'qwen' in name.lower():
                            print(f"     âœ… æ‰¾åˆ°qwenæ¨¡å‹!")
                else:
                    print("   âš ï¸  æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ¨¡å‹")
                return url
            else:
                print(f"âŒ HTTPé”™è¯¯: {response.status_code}")
        except requests.exceptions.ConnectionError as e:
            print(f"âŒ è¿æ¥é”™è¯¯: æ— æ³•è¿æ¥åˆ°æœåŠ¡")
        except requests.exceptions.Timeout as e:
            print(f"âŒ è¶…æ—¶é”™è¯¯: è¿æ¥è¶…æ—¶")
        except Exception as e:
            print(f"âŒ å…¶ä»–é”™è¯¯: {e}")
    
    print(f"\nâŒ æ— æ³•è¿æ¥åˆ°ä»»ä½•OllamaæœåŠ¡")
    return None

def check_docker_containers():
    """æ£€æŸ¥Dockerå®¹å™¨çŠ¶æ€"""
    print(f"\nğŸ³ æ£€æŸ¥Dockerå®¹å™¨çŠ¶æ€")
    print("=" * 30)
    
    try:
        import subprocess
        result = subprocess.run(['docker', 'ps', '--filter', 'name=ollama'], 
                              capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            output = result.stdout
            if 'ollama' in output.lower():
                print("âœ… æ‰¾åˆ°Ollamaå®¹å™¨:")
                lines = output.strip().split('\n')
                for line in lines[1:]:  # è·³è¿‡æ ‡é¢˜è¡Œ
                    if 'ollama' in line.lower():
                        print(f"   {line}")
            else:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°è¿è¡Œä¸­çš„Ollamaå®¹å™¨")
        else:
            print(f"âŒ Dockerå‘½ä»¤æ‰§è¡Œå¤±è´¥: {result.stderr}")
    except subprocess.TimeoutExpired:
        print("âŒ Dockerå‘½ä»¤æ‰§è¡Œè¶…æ—¶")
    except FileNotFoundError:
        print("âŒ æœªæ‰¾åˆ°Dockerå‘½ä»¤ï¼Œè¯·ç¡®ä¿Dockerå·²å®‰è£…")
    except Exception as e:
        print(f"âŒ æ£€æŸ¥Dockerå®¹å™¨æ—¶å‡ºé”™: {e}")

def test_ollama_client():
    """æµ‹è¯•OllamaClient"""
    print(f"\nğŸ§ª æµ‹è¯•OllamaClient")
    print("=" * 30)
    
    try:
        from llm.ollama_client import OllamaClient
        
        client = OllamaClient()
        print(f"âœ… OllamaClientåˆå§‹åŒ–æˆåŠŸ")
        print(f"ğŸ“ API URL: {client.api_url}")
        print(f"ğŸ¤– LLMæ¨¡å‹: {client.llm_model}")
        print(f"ğŸ”¤ åµŒå…¥æ¨¡å‹: {client.embed_model}")
        
        # æµ‹è¯•embedding
        print(f"\nğŸ”¤ æµ‹è¯•embeddingåŠŸèƒ½...")
        try:
            embedding = client.get_embedding("æµ‹è¯•æ–‡æœ¬")
            print(f"âœ… EmbeddingæˆåŠŸï¼Œå‘é‡ç»´åº¦: {len(embedding)}")
        except Exception as e:
            print(f"âŒ Embeddingå¤±è´¥: {e}")
        
        # æµ‹è¯•ç”Ÿæˆ
        print(f"\nğŸ’¬ æµ‹è¯•ç”ŸæˆåŠŸèƒ½...")
        try:
            response = client.generate_response("ä½ å¥½ï¼Œè¯·ç®€å•å›ç­”")
            print(f"âœ… ç”ŸæˆæˆåŠŸ")
            print(f"ğŸ“ å“åº”: {response[:100]}...")
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
            
    except Exception as e:
        print(f"âŒ OllamaClientåˆå§‹åŒ–å¤±è´¥: {e}")

def provide_solutions():
    """æä¾›è§£å†³æ–¹æ¡ˆ"""
    print(f"\nğŸ’¡ è§£å†³æ–¹æ¡ˆå»ºè®®")
    print("=" * 50)
    
    print("1. ğŸ”§ ç¡®ä¿Ollamaå®¹å™¨æ­£åœ¨è¿è¡Œ:")
    print("   docker-compose up -d ollama_service")
    print()
    
    print("2. ğŸ“¥ ç¡®ä¿qwenæ¨¡å‹å·²ä¸‹è½½:")
    print("   docker exec -it ollama_host ollama pull qwen2.5:3b-instruct")
    print("   docker exec -it ollama_host ollama pull bge-m3")
    print()
    
    print("3. ğŸ” æ£€æŸ¥å®¹å™¨æ—¥å¿—:")
    print("   docker logs ollama_host")
    print()
    
    print("4. ğŸŒ æµ‹è¯•ç«¯å£è¿æ¥:")
    print("   curl http://localhost:11434/api/tags")
    print()
    
    print("5. ğŸ”„ é‡å¯OllamaæœåŠ¡:")
    print("   docker-compose restart ollama_service")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Ollamaè¿æ¥è¯Šæ–­å·¥å…·")
    print("=" * 60)
    
    # æ£€æŸ¥Dockerå®¹å™¨
    check_docker_containers()
    
    # æ£€æŸ¥Ollamaè¿æ¥
    working_url = check_ollama_connection()
    
    if working_url:
        # å¦‚æœè¿æ¥æˆåŠŸï¼Œæµ‹è¯•OllamaClient
        test_ollama_client()
    else:
        # å¦‚æœè¿æ¥å¤±è´¥ï¼Œæä¾›è§£å†³æ–¹æ¡ˆ
        provide_solutions()
    
    print(f"\n" + "=" * 60)
    print("ğŸ è¯Šæ–­å®Œæˆ")

if __name__ == "__main__":
    main()